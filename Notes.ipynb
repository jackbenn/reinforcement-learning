{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89244468",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292e676",
   "metadata": {},
   "source": [
    "## Terms\n",
    "\n",
    "The **state** of a system encompessed all information needed to make a decision. In particular, in **Markov** systems, remembering previous states won't help.\n",
    "\n",
    "A **policy** connects states to actions, giving the probability that an action will be taken at any given state.\n",
    "\n",
    "An **on-policy** method follows the same policy that is being learned about. An **off-policy** method, on the other hand, has a different **behavior policy** and **target policy**.\n",
    "\n",
    "**Prediction** methods are used to estimate the value of a given state (or action-state pair) under a given policy. **Control** methods try to determine the optimal policy.\n",
    "\n",
    "\n",
    "A **model** provides information about rewards and next states given states and actions. A **distribution model** gives the distribution of results; a **sample model** just samples from that distribution.\n",
    "\n",
    "**Planning** takes us from a model toward an optimal policy.\n",
    "\n",
    "**Learning** is the process of gaining information from the environment. **Model learning** is improving the model to make it match the environment; **direct reinforcement learning** is improving the value function or policy directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b9028",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "**SARSA** (standing for **S**tate-**A**ction-**R**eward-**S**tate-**A**ction an control-learning method in which, after each step, q-values from the prior state-action are updated in the direction of the latest reward minor plus the difference of the current state-action minus the prior one.\n",
    "\n",
    "**Expected SARSA** reduces the variance of SARSA this by using the expected q-value from the current state following the policy, rather than the q-value for the state that was actually taken. This can be used as off-policy approach, with a different policy for the expectation than the one followed.\n",
    "\n",
    "**Q-Learning** can be seen as a special case of Expected SARSA in which the target policy is greedy.\n",
    "\n",
    "**Dyna-Q** combines direct RL, model learning, and planning. The direct RL and model learning are both tabular, with RL being Q-learning and the model learning assuming a deterministic model. The planning does Q-learning based on random sampling.\n",
    "\n",
    "**Dyna-Q+** gives a bonus reward when exploring based on how long since a state-action pair has been tried. (how?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8a3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
